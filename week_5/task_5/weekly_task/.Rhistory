apply(project,1,add_in_likecount)
add_in_likecount <- function(project){
flag = (add_up$project_name == project['project_name'])
add_up$count[flag] = add_up$count[flag] + as.numeric(project['count'])
add_up$count
}
apply(project,1,add_in_likecount)
add_in_likecount <- function(project){
flag = (add_up$project_name == project['project_name'])
add_up$count[flag] = add_up$count[flag] + as.numeric(project['count'])
add_up$count[flag]
}
apply(project,1,add_in_likecount)
test = list(1,2,3)
test
test[1]
test = c(1,2,3)
test
test[1]
test[2]
add_in_likecount <- function(project){
flag = (add_up$project_name == project['project_name'])
add_up$count[flag] = (add_up$count[flag] + as.numeric(project['count']))
return(add_up)
}
apply(project,1,add_in_likecount)
add_in_likecount <- function(project){
flag = (add_up$project_name == project['project_name'])
add_up$count[flag] = (add_up$count[flag] + as.numeric(project['count']))
return(add_up$count)
}
add_in_likecount <- function(project){
flag = (add_up$project_name == project['project_name'])
add_up$count[flag] = (add_up$count[flag] + as.numeric(project['count']))
return(add_up$count)
}
sum(apply(project,1,add_in_likecount))
apply(project,1,add_in_likecount)
merge(c(1,0,0),c(0,1,0))
c(1,0,0)+c(0,1,0)
sum(c(1,0,0),c(0,1,0))
add_up$count = add_up$count+apply(project,1,add_in_likecount)
add_up$count
add_up
url = "https://www.facebook.com/zeczec.com"
my_token = "EAACEdEose0cBAMH49NmSyhUXup313fxUXZBAtmSXZBy7Hw4RVpQ48f6znFuTskSABiOLSCLnGTQGPZCJCBGQ0oigqaBdmi6e3pIoVHZAfJVqVUMY0S1eheGRoBpssZCxEKTVj0XuKxPGjX1QzJRBWPLeN0mcJ9VKn2ZCssfZAk3jRcsKrF0rQRr61JNp1peRwDG2hi0ZBF8arwZDZD"
me = getUsers("me", my_token, private_info = T)
page_name = "嘖嘖 zeczec"
page_id = 197008103698657
target = getPage(page_id, my_token, n = 75)
View(target$message[1])
flag = grepl("projects/",target$message)
num = regexec("projects/",target$message[flag])
len = nchar(target$message[flag])
project_name = substr(target$message[flag],as.numeric(num) + nchar("projects/"),len)
count = target$likes_count[flag]
flag = grepl("\n",project_name)
num = regexec("\n",project_name[flag])
project_name[flag] = substr(project_name[flag],1,as.numeric(num)-1)
project = data.frame(project_name,count)
add_up = data.frame(unique(project$project_name),0)
names(add_up) <- c("project_name" ,"count")
library(httr)
library(rjson)
library(httpuv)
library(Rfacebook)
library(stringr)
url = "https://www.facebook.com/zeczec.com"
my_token = "EAACEdEose0cBAEZBZC0N8lZAG1uqgbE8TUMkxgZA2iHUiku4YrLu2vyhvLvN1ZCyVfN2mXdueUycK3wZBsPdxcsXLWYiXPNSiM5b5OrJ721YLD47s207fEHFpKnn1LWEistEeNcFFGpEAoER7jZBZCCN6sIroG7QOSPII2WwIiPQ6udzWStirMg2z3QPG4mWflHNTdHkXGx10wZDZD"
me = getUsers("me", my_token, private_info = T)
page_name = "嘖嘖 zeczec"
page_id = 197008103698657
target = getPage(page_id, my_token, n = 75)
flag = grepl("projects/",target$message)
num = regexec("projects/",target$message[flag])
len = nchar(target$message[flag])
project_name = substr(target$message[flag],as.numeric(num) + nchar("projects/"),len)
count = target$likes_count[flag]
flag = grepl("\n",project_name)
num = regexec("\n",project_name[flag])
project_name[flag] = substr(project_name[flag],1,as.numeric(num)-1)
project = data.frame(project_name,count)
add_up = data.frame(unique(project$project_name),0)
names(add_up) <- c("project_name" ,"count")
add_in_likecount <- function(project){
flag = (add_up$project_name == project['project_name'])
add_up$count[flag] = add_up$count[flag] + as.numeric(project['count'])
}
apply(project,1,add_in_likecount)
add_up
temp = apply(project,1,add_in_likecount)
temp
add_in_likecount <- function(project){
flag = (add_up$project_name == project['project_name'])
add_up$count[flag] = add_up$count[flag] + as.numeric(project['count'])
add_up$count[flag]
}
temp = apply(project,1,add_in_likecount)
temp = apply(project,1,add_in_likecount)
temp
add_in_likecount <- function(project){
flag = (add_up$project_name == project['project_name'])
add_up$count[flag] = add_up$count[flag] + as.numeric(project['count'])
print(add_up$count[flag])
}
temp = apply(project,1,add_in_likecount)
project
project[1]
project[,1]
project[1,]
add_up$project_name == project[1,]$project_name
add_up[add_up$project_name == project[1,]$project_name]
add_up[add_up$project_name == project[1,]$project_name,2]
add_up[add_up$project_name == project[1,]$project_name,]
for( i in c(1:75)){
flag = add_up$project_name == project[i,1]
add_up[flag,2] = add_up[flag, 2] + project[i,2]
}
add_up
install.packages("wordcloud")
add_up
target
target$message
add_up
library(wordcloud)
library(RColorBrewer)
wordcloud(add_up$project_name,add_up$count,scale = c(10,.5),min.freq = 5,max.freq = Inf)
warnings()
wordcloud(add_up$project_name,add_up$count,scale = c(10,.3),min.freq = 5)
warnings()
wordcloud(add_up$project_name,add_up$count,scale = c(20,.5),min.freq = 5)
wordcloud(add_up$project_name,add_up$count,scale = c(10,.5),min.freq = 5)
wordcloud(add_up$project_name,add_up$count,scale = c(10,.3),min.freq = 5)
wordcloud(add_up$project_name,add_up$count,scale = c(10,.1),min.freq = 5)
wordcloud(add_up$project_name,add_up$count,scale = c(20,.5),min.freq = 5)
wordcloud(add_up$project_name,add_up$count,scale = c(10,.1),min.freq = 5)
wordcloud(add_up$project_name,add_up$count,scale = c(3,.1),min.freq = 5)
wordcloud(add_up$project_name,add_up$count/2,scale = c(3,.1),min.freq = 5)
wordcloud(add_up$project_name,add_up$count/4,scale = c(3,.1),min.freq = 5)
wordcloud(add_up$project_name,add_up$count/4,scale = c(3,.01),min.freq = 5)
wordcloud(add_up$project_name,add_up$count/4,scale = c(3,.04),min.freq = 5)
wordcloud(add_up$project_name,add_up$count/4,scale = c(3,.04),min.freq = 10)
View(add_up)
temp = wordcloud(add_up$project_name,add_up$count/4,scale = c(3,.04),min.freq = 10)
View(temp)
temp <- wordcloud(add_up$project_name,add_up$count/4,scale = c(3,.04),min.freq = 10)
View(temp)
temp <- wordcloud(add_up$project_name,add_up$count/4,scale = c(3,.04),min.freq = 10,rot.per = .1)
temp <- wordcloud(add_up$project_name,add_up$count/4,scale = c(5,.1),min.freq = 10,rot.per = .1)
temp <- wordcloud(add_up$project_name,add_up$count/4,scale = c(3,.1),min.freq = 5,rot.per = .1)
add_up
add_up[add_up$count >50]
add_up[add_up$count > 50]
add_up$count > 50
add_up$count > 5
add_up[add_up$count > 5,]
summary(target)
target$shares_count
target$likes_count
library(NLP)
library(readtext)
library(tm)
library(jiebaR)
library(jiebaRD)
library(xml2)
library(rvest)
library(stringr)
#custom option===
end_index = 2360        #終止頁數#2360為2018年的開始頁數
latest_index = 2415     #最新頁數
#================程式將會從最新頁數一直抓到終止頁數
#關鍵字設定請看第25行
pre_url = "https://www.ptt.cc/bbs/NTU/index"
html_url = ".html"
#post_count = 0
#guan_count = 0
ptt_text = c()    #這是我要的文字內容，全部丟在這個vector裡面最後再做輸出
for(i in c(latest_index:end_index)){
url <- paste0(pre_url,i,html_url)                #建立url進入看板NTU
links_data_title <- read_html(url) %>% html_nodes(".title") %>% html_text()        #抓取標題
links_data_url <- read_html(url) %>% html_nodes(".title a") %>% html_attr('href')  #抓取標題的連結
#post_count = post_count + length(links_data_title)
links_data_title <- str_replace_all(links_data_title,"\t","") %>% str_replace_all("\n","")
#custom setting========
flag = xor(grepl("管",links_data_title)|grepl("校長",links_data_title),      #包含的關鍵字
grepl("管理",links_data_title)|grepl("管學院",links_data_title))  #除去的關鍵字
#======================程式會根據標題裡面的關鍵字做抓取
#guan_count = guan_count + length(links_data_title[flag])
for(j in (1:length(links_data_title))[flag]){
links_url = paste0('https://www.ptt.cc',links_data_url[j])
content_css = read_html(links_url) %>% html_nodes("#main-content") %>% html_text()
ptt_text = c(ptt_text, iconv(content_css,'utf8'))
}
}                                                                              #進入每一個貼文中抓取文字
ptt_text
docs
ptt_text
data.corpus = Corpus(VectorSource(ptt_text))
data.corpus
data.corpus = tm_map(data.corpus, removePunctuation)
data.corpus = tm_map(data.corpus, removeNumbers)
data.corpus = tm_map(data.corpus, function(word){
gsub("[\n\ta-zA   -Z，。、？！：～ ※→]","",word)
})
content(data.corpus)
mixseg = worker()
jieba_tokenizer = function(d)
{
unlist( segment(d[[1]], mixseg) )
}
seg = lapply(d.corpus, jieba_tokenizer)
mixseg = worker()
jieba_tokenizer = function(d)
{
unlist( segment(d[[1]], mixseg) )
}
seg = lapply(data.corpus, jieba_tokenizer)
seg
count_token = function(d)
{
as.data.frame(table(d))
}
tokens = lapply(seg, count_token)
tokens
tokens[2]
data.corpus = Corpus(VectorSource(ptt_text))
data.corpus = tm_map(data.corpus, removePunctuation)
data.corpus = tm_map(data.corpus, removeNumbers)
data.corpus = tm_map(data.corpus, function(word){gsub("[噓推的了\n\ta-zA-Z，。、？！：～ ※→]"," ",word) })
mixseg = worker()
keywords = read.csv("keywords.csv",fileEncoding = 'big5')
new_user_word(mixseg, as.matrix(keywords))
jieba_tokenizer = function(d)
{
unlist( segment(d[[1]], mixseg) )
}
seg = lapply(data.corpus, jieba_tokenizer)
count_token = function(d)
{
as.data.frame(table(d))
}
tokens = lapply(seg, count_token)
data.corpus = Corpus(VectorSource(ptt_text))
data.corpus = tm_map(data.corpus, removePunctuation)
data.corpus = tm_map(data.corpus, removeNumbers)
data.corpus = tm_map(data.corpus, function(word){gsub("[噓推的了\n\ta-zA-Z，。、？！：～ ※→]"," ",word) })
mixseg = worker()
keywords = read.csv("keywords.csv",fileEncoding = 'big5')
new_user_word(mixseg, as.matrix(keywords))
jieba_tokenizer = function(d)
{
unlist( segment(d[[1]], mixseg) )
}
seg = lapply(data.corpus, jieba_tokenizer)
count_token = function(d)
{
as.data.frame(table(d))
}
tokens = lapply(seg, count_token)
setwd("~/Desktop/CSX_4001_106-2/week_5/task_5/weekly_task")
data.corpus = Corpus(VectorSource(ptt_text))
data.corpus = tm_map(data.corpus, removePunctuation)
data.corpus = tm_map(data.corpus, removeNumbers)
data.corpus = tm_map(data.corpus, function(word){gsub("[噓推的了\n\ta-zA-Z，。、？！：～ ※→]"," ",word) })
mixseg = worker()
keywords = read.csv("keywords.csv",fileEncoding = 'big5')
new_user_word(mixseg, as.matrix(keywords))
jieba_tokenizer = function(d)
{
unlist( segment(d[[1]], mixseg) )
}
seg = lapply(data.corpus, jieba_tokenizer)
count_token = function(d)
{
as.data.frame(table(d))
}
tokens = lapply(seg, count_token)
keywords = read.csv("keywords.csv",fileEncoding = 'big5',quote = "")
keywords = read.csv("keywords.csv",quote = "")
new_user_word(mixseg, as.matrix(keywords))
jieba_tokenizer = function(d)
{
unlist( segment(d[[1]], mixseg) )
}
seg = lapply(data.corpus, jieba_tokenizer)
count_token = function(d)
{
as.data.frame(table(d))
}
tokens = lapply(seg, count_token)
tokens
token
tokens
View(tokens)
View(tokens[[1]])
View(tokens[[2]])
col = names(seg)
col
length(seg)
Merge_token = token[[1]]
Merge_token = tokens[[1]]
Merge_token
str(seg)
names(seg)
str(tokens)
names(tokens)
length(seg)
tokens[1]
tokens[[1]]
len = length(seg)
Merge_tokens = tokens[[1]]
for( id in c(2:n) )
{
Merge_tokens = merge(Merge_tokens, tokens[[id]], by="d", all = TRUE)
}
len = length(seg)
Merge_tokens = tokens[[1]]
for( id in c(2:len) )
{
Merge_tokens = merge(Merge_tokens, tokens[[id]], by="d", all = TRUE)
}
warnings()
len = length(seg)
Merge_tokens = tokens[[1]]
for( id in c(2:len) )
{
Merge_tokens = merge(Merge_tokens, tokens[[id]], by="d", all = TRUE)
names(Merge_tokens) = c("d","Freq")
}
names(Merge_tokens)
tokens[[1]]
len = length(seg)
Merge_tokens = tokens[[1]]
for( id in c(2:len) )
{
Merge_tokens = merge(Merge_tokens, tokens[[id]], by="d", all = TRUE)
names(Merge_tokens) = c("d","Freq")
}
names(Merge_tokens)=c("d","Freq")
names(Merge_tokens)
len = length(seg)
Merge_tokens = tokens[[1]]
names(Merge_tokens)
Merge_tokens = merge(Merge_tokens, tokens[[2]], by="d", all = TRUE)
names(Merge_tokens)
Merge_tokens = merge(Merge_tokens, tokens[[3]], by="d", all = TRUE)
names(Merge_tokens)
Merge_tokens
as.character(1:2)
len = length(seg)
Merge_tokens = tokens[[1]]
for( id in c(2:len) )
{
Merge_tokens = merge(Merge_tokens, tokens[[id]], by="d", all = TRUE)
names(Merge_tokens) = c("d",as.character(1:id))
}
Merge_tokens
Merge_tokens[is.na(Merge_tokens)] = 0
Merge_tokens
sum(Merge_tokens[,2:179])
Merge_tokens[,1]
Merge_tokens[1,]
sum(Merge_tokens[1,2:179])
sum(Merge_tokens[1,2:178])
len
Merge_tokens[1]
Merge_tokens[2]
Merge_tokens[,2]
Merge_tokens[,1]
Sum_up = function(freq){
return(sum(freq[,2:(len+1)]))
}
Sum_up(Merge_tokens)
lapply(Merge_tokens,Sum_up)
Sum_up = function(freq){
return(sum(freq[2:(len+1)]))
}
lapply(Merge_tokens,Sum_up)
data.frame(c(1,2,3),c(4:6))
Sum_up = function(freq){
print(freq[2:len+1])
return(sum(freq[2:(len+1)]))
}
lapply(Merge_tokens,Sum_up)
lapply(Merge_tokens,Sum_up,n=2)
lapply(Merge_tokens,Sum_up,2)
lapply(Merge_tokens,2,Sum_up)
apply(Merge_tokens,1,Sum_up)
apply(Merge_tokens,2,Sum_up)
apply(Merge_tokens,1,Sum_up)
Sum_up = function(freq){
print(freq[2:len+1])
return(sum(freq[2:(len+1)]))
}
apply(Merge_tokens,1,Sum_up)
str(Merge_tokens$freq)
str(Merge_tokens$`1`)
lapply(Merge_tokens,1,Sum_up)
lapply(Merge_tokens,Sum_up)
Merge_tokens
help(mapply)
rowsum(Merge_tokens)
rowSums(Merge_tokens)
rowSums(Merge_tokens[,2:len+1])
Merge_token$Sum= rowSums(Merge_tokens[,2:len+1])
length(Merge_tokens)
length(Merge_tokens$d)
sum = rowSums(Merge_tokens[,2:len+1])
sum
length(sum)
Merge_token$Sum= c(rowSums(Merge_tokens[,2:len+1]))
append(Merge_tokens,sum)
names(Merge_tokens)
str(sum)
str(Merge_tokens)
str(tail(Merge_tokens))
tail(Merge_tokens)
length(sum)
rowSums(Merge_tokens[,2:len+1])
sum = rowSums(Merge_tokens[,2:len+1])
length(sum)
Merge_tokens$Sum = sum
len = length(seg)
Merge_tokens = tokens[[1]]
for( id in c(2:len) )
{
Merge_tokens = merge(Merge_tokens, tokens[[id]], by="d", all = TRUE)
names(Merge_tokens) = c("d",as.character(1:id))
}
Merge_tokens[is.na(Merge_tokens)] = 0
sum = rowSums(Merge_tokens[,2:len+1])
Merge_token$Sum = sum
sum
length(sum)
length(Merge_tokens$d)
sum = rowSums(Merge_tokens[,2:len+1])
Merge_tokens$Sum = sum
sort(Merge_tokens$Sum)
sort(Merge_tokens$Sum,decreasing = T)
sort(Merge_tokens,by = "Sum",decreasing = T)
subset(Merge_tokens,c("d","Sum"))
Merge_tokens[1,]
Merge_tokens[,"d"]
Merge_tokens[,c("d","Sum")]
Result = Merge_tokens[,c("d","Sum")]
Result
library(dplyr)
order(Result,decreasing = T)
Result[order(Sum),]
Result[order(Result$Sum),]
Result[order(Result$Sum,decreasing = T),]
library(knitr)
kable(head(Result[order(Result$Sum,decreasing = T),]))
kable(head(Result[order(Result$Sum,decreasing = T),]),tail(Result[order(Result$Sum,decreasing = T),]))
Result
Result[order(Result$Sum,decreasing = T),]
install.packages("Matix")
install.packages("Matrix")
install.packages("Matrix")
library(Matrix)
library(Matrix)
idfCal <- function(word_doc)
{
log2( n / nnzero(word_doc) )
}
idf <- apply(as.matrix(Merge_tokens[,2:(len+1)]), 1, idfCal)
doc.tfidf <- Merge_tokens
tempY = matrix(rep(c(as.matrix(tf)), each = length(idf)), nrow = length(idf))
tempX = matrix(rep(c(as.matrix(idf)), each = length(tf)), ncol = length(tf), byrow = TRUE)
doc.tfidf[,2:(len+1)] <- (doc.tfidf[,2:(len+1)] / tempY) * tempX
stopLine = rowSums(doc.tfidf[,2:(len+1)])
delID = which(stopLine == 0)
kable(head(doc.tfidf[delID,1]))
len = length(seg)
Merge_tokens = tokens[[1]]
for( id in c(2:len) )
{
Merge_tokens = merge(Merge_tokens, tokens[[id]], by="d", all = TRUE)
names(Merge_tokens) = c("d",as.character(1:id))
}
Merge_tokens[is.na(Merge_tokens)] = 0
tf = rowSums(Merge_tokens[,2:len+1])
library(Matrix)
idfCal <- function(word_doc)
{
log2( n / nnzero(word_doc) )
}
idf <- apply(as.matrix(Merge_tokens[,2:(len+1)]), 1, idfCal)
library(Matrix)
idfCal <- function(word_doc)
{
log2( len / nnzero(word_doc) )
}
idf <- apply(as.matrix(Merge_tokens[,2:(len+1)]), 1, idfCal)
idf
doc.tfidf <- Merge_tokens
tempY = matrix(rep(c(as.matrix(tf)), each = length(idf)), nrow = length(idf))
tempX = matrix(rep(c(as.matrix(idf)), each = length(tf)), ncol = length(tf), byrow = TRUE)
doc.tfidf[,2:(len+1)] <- (doc.tfidf[,2:(len+1)] / tempY) * tempX
stopLine = rowSums(doc.tfidf[,2:(len+1)])
delID = which(stopLine == 0)
kable(head(doc.tfidf[delID,1]))
delID
stopLine
idf
doc.tfidf
install.packages("tmcn")
install.packages("Rwordseg")
library(Rwordseg)
install.package("Rwordseg")
install.packages(Rwordseg)
install.packages("Rwordseg")
