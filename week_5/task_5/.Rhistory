latest_index = 2366
post_count = 0
guan_count = 0
ptt_text = c()
for(i in c(latest_index:end_index)){
url = cat(pre_url,i,html_url,sep = "")
links_data_title <- read_html(url) %>% html_nodes(".title") %>% html_text()
links_data_url <- read_html(url) %>% html_nodes(".title a") %>% html_attr('href')
post_count = post_count + length(links_data_title)
links_data_title %>% str_replace_all("\t","") %>% str_replace_all("\n","")
flag = grepl("管",links_data_title)|grepl("校長",links_data_title)
guan_count = guan_count + length(links_data_title[flag])
for(j in 1:length(links_data_title[flag])){
links_url = paste0('https://www.ptt.cc',links_data_url[j])
content_css = read_html(links_url) %>% html_nodes("#main-content") %>% html_text()
ptt_text = c(ptt_text, iconv(content_css,'utf8'))
}
}
# 1. import data
pre_url = "https://www.ptt.cc/bbs/NTU/index"
html_url = ".html"
end_index = 2360
latest_index = 2366
post_count = 0
guan_count = 0
ptt_text = c()
for(i in c(latest_index:end_index)){
url = cat(pre_url,i,html_url,sep = "")
links_data_title <- read_html(url) %>% html_nodes(".title") %>% html_text()
links_data_url <- read_html(url) %>% html_nodes(".title a") %>% html_attr('href')
post_count = post_count + length(links_data_title)
links_data_title %>% str_replace_all("\t","") %>% str_replace_all("\n","")
flag = grepl("管",links_data_title)|grepl("校長",links_data_title)
guan_count = guan_count + length(links_data_title[flag])
for(j in 1:length(links_data_title[flag])){
links_url = paste0('https://www.ptt.cc',links_data_url[j])
content_css = read_html(links_url) %>% html_nodes("#main-content") %>% html_text()
ptt_text = c(ptt_text, iconv(content_css,'utf8'))
}
}
latest_index
end_index
latest_index:end_index
for(i in c(latest_index:end_index)){
print(i)
}
i
url = cat(pre_url,i,html_url,sep = "")
links_data_title <- read_html(url) %>% html_nodes(".title") %>% html_text()
links_data_title <- read_html(url) #%>% html_nodes(".title") %>% html_text()
library(rvest)
url
url = cat(pre_url,i,html_url,sep = "")
url
url <- cat(pre_url,i,html_url,sep = "")
url
url <- paste0(pre_url,i,html_url)
url
links_data_title <- read_html(url) #%>% html_nodes(".title") %>% html_text()
links_data_tilte
links_data_title
for(i in c(latest_index:end_index)){
url <- paste0(pre_url,i,html_url)
links_data_title <- read_html(url) %>% html_nodes(".title") %>% html_text()
links_data_url <- read_html(url) %>% html_nodes(".title a") %>% html_attr('href')
post_count = post_count + length(links_data_title)
links_data_title %>% str_replace_all("\t","") %>% str_replace_all("\n","")
flag = grepl("管",links_data_title)|grepl("校長",links_data_title)
guan_count = guan_count + length(links_data_title[flag])
for(j in 1:length(links_data_title[flag])){
links_url = paste0('https://www.ptt.cc',links_data_url[j])
content_css = read_html(links_url) %>% html_nodes("#main-content") %>% html_text()
ptt_text = c(ptt_text, iconv(content_css,'utf8'))
}
}
ptt_text
T+F
F+F
F+T
T+T
links_data_tilte
links_data_title
links_data_title %>% str_replace_all("\t","") %>% str_replace_all("\n","")
links_data_title
links_data_title <- str_replace_all("\t","") %>% str_replace_all("\n","")
links_data_title <- str_replace_all(links_data_title"\t","") %>% str_replace_all("\n","")
links_data_title <- str_replace_all(links_data_title,"\t","") %>% str_replace_all("\n","")
links_data_title
flag = grepl("管",links_data_title)|grepl("校長",links_data_title)
flag
url
url = "https://www.ptt.cc/bbs/NTU/index2340.html"
links_data_title <- read_html(url) %>% html_nodes(".title") %>% html_text()
links_data_url <- read_html(url) %>% html_nodes(".title a") %>% html_attr('href')
links_data_url
#post_count = post_count + length(links_data_title)
links_data_title <- str_replace_all(links_data_title,"\t","") %>% str_replace_all("\n","")
links_data_title
flag = grepl("管",links_data_title)|grepl("校長",links_data_title)
flag
TxorF
xor(T,F)
xor(c(T,T),c(F,T))
#custom option===
end_index = 2360        #終止頁數
latest_index = 2366     #最新頁數
#================程式將會從最新頁數一直抓到終止頁數
#關鍵字設定請看第31行
pre_url = "https://www.ptt.cc/bbs/NTU/index"
html_url = ".html"
#post_count = 0
guan_count = 0
ptt_text = c()
for(i in c(latest_index:end_index)){
url <- paste0(pre_url,i,html_url)
links_data_title <- read_html(url) %>% html_nodes(".title") %>% html_text()
links_data_url <- read_html(url) %>% html_nodes(".title a") %>% html_attr('href')
#post_count = post_count + length(links_data_title)
links_data_title <- str_replace_all(links_data_title,"\t","") %>% str_replace_all("\n","")
#custom setting========
flag = xor(grepl("管",links_data_title)|grepl("校長",links_data_title),      #包含的關鍵字
grepl("管理",links_data_title)|grepl("管學院",links_data_title))  #除去的關鍵字
#======================程式會根據標題裡面的關鍵字做抓取
#guan_count = guan_count + length(links_data_title[flag])
for(j in 1:length(links_data_title[flag])){
links_url = paste0('https://www.ptt.cc',links_data_url[j])
content_css = read_html(links_url) %>% html_nodes("#main-content") %>% html_text()
ptt_text = c(ptt_text, iconv(content_css,'utf8'))
}
}
ptt_text
write.table(ptt_text, file = "text_content.txt", sep = " ", fileEncoding = 'big5')
write.table(ptt_text, file = "text_content.txt", sep = " ")
setwd("~/Desktop/CSX_4001_106-2/week_5/task_5/weekly_task")
write.table(ptt_text, file = "text_content.txt", sep = " ")
library(xml2)
library(rvest)
library(stringr)
#custom option===
end_index = 2360        #終止頁數#2360為2018年的開始頁數
latest_index = 2460     #最新頁數
#================程式將會從最新頁數一直抓到終止頁數
#關鍵字設定請看第31行
pre_url = "https://www.ptt.cc/bbs/NTU/index"
html_url = ".html"
#post_count = 0
guan_count = 0
ptt_text = c()
for(i in c(latest_index:end_index)){
url <- paste0(pre_url,i,html_url)
links_data_title <- read_html(url) %>% html_nodes(".title") %>% html_text()
links_data_url <- read_html(url) %>% html_nodes(".title a") %>% html_attr('href')
#post_count = post_count + length(links_data_title)
links_data_title <- str_replace_all(links_data_title,"\t","") %>% str_replace_all("\n","")
#custom setting========
flag = xor(grepl("管",links_data_title)|grepl("校長",links_data_title),      #包含的關鍵字
grepl("管理",links_data_title)|grepl("管學院",links_data_title))  #除去的關鍵字
#======================程式會根據標題裡面的關鍵字做抓取
#guan_count = guan_count + length(links_data_title[flag])
for(j in 1:length(links_data_title[flag])){
links_url = paste0('https://www.ptt.cc',links_data_url[j])
content_css = read_html(links_url) %>% html_nodes("#main-content") %>% html_text()
ptt_text = c(ptt_text, iconv(content_css,'utf8'))
}
}
write.table(ptt_text, file = "text_content.txt", sep = " ")
library(xml2)
library(rvest)
library(stringr)
#custom option===
end_index = 2360        #終止頁數#2360為2018年的開始頁數
latest_index = 2460     #最新頁數
#================程式將會從最新頁數一直抓到終止頁數
#關鍵字設定請看第31行
pre_url = "https://www.ptt.cc/bbs/NTU/index"
html_url = ".html"
#post_count = 0
guan_count = 0
ptt_text = c()
for(i in c(latest_index:end_index)){
url <- paste0(pre_url,i,html_url)
links_data_title <- read_html(url) %>% html_nodes(".title") %>% html_text()
links_data_url <- read_html(url) %>% html_nodes(".title a") %>% html_attr('href')
#post_count = post_count + length(links_data_title)
links_data_title <- str_replace_all(links_data_title,"\t","") %>% str_replace_all("\n","")
#custom setting========
flag = xor(grepl("管",links_data_title)|grepl("校長",links_data_title),      #包含的關鍵字
grepl("管理",links_data_title)|grepl("管學院",links_data_title))  #除去的關鍵字
#======================程式會根據標題裡面的關鍵字做抓取
#guan_count = guan_count + length(links_data_title[flag])
for(j in 1:length(links_data_title[flag])){
links_url = paste0('https://www.ptt.cc',links_data_url[j])
content_css = read_html(links_url) %>% html_nodes("#main-content") %>% html_text()
ptt_text = c(ptt_text, iconv(content_css,'utf8'))
}
}
write.table(ptt_text, file = "text_content.txt", sep = " ")
library(xml2)
library(rvest)
library(stringr)
#custom option===
end_index = 2360        #終止頁數#2360為2018年的開始頁數
latest_index = 2460     #最新頁數
#================程式將會從最新頁數一直抓到終止頁數
#關鍵字設定請看第31行
pre_url = "https://www.ptt.cc/bbs/NTU/index"
html_url = ".html"
#post_count = 0
guan_count = 0
ptt_text = c()
for(i in c(latest_index:end_index)){
url <- paste0(pre_url,i,html_url)
links_data_title <- read_html(url) %>% html_nodes(".title") %>% html_text()
links_data_url <- read_html(url) %>% html_nodes(".title a") %>% html_attr('href')
#post_count = post_count + length(links_data_title)
links_data_title <- str_replace_all(links_data_title,"\t","") %>% str_replace_all("\n","")
#custom setting========
flag = xor(grepl("管",links_data_title)|grepl("校長",links_data_title),      #包含的關鍵字
grepl("管理",links_data_title)|grepl("管學院",links_data_title))  #除去的關鍵字
#======================程式會根據標題裡面的關鍵字做抓取
#guan_count = guan_count + length(links_data_title[flag])
for(j in 1:length(links_data_title[flag])){
links_url = paste0('https://www.ptt.cc',links_data_url[j])
content_css = read_html(links_url) %>% html_nodes("#main-content") %>% html_text()
ptt_text = c(ptt_text, iconv(content_css,'utf8'))
}
}
write.table(ptt_text, file = "text_content.txt", sep = " ")
library(xml2)
library(rvest)
library(stringr)
#custom option===
end_index = 2360        #終止頁數#2360為2018年的開始頁數
latest_index = 2460     #最新頁數
#================程式將會從最新頁數一直抓到終止頁數
#關鍵字設定請看第31行
pre_url = "https://www.ptt.cc/bbs/NTU/index"
html_url = ".html"
#post_count = 0
guan_count = 0
ptt_text = c()
for(i in c(latest_index:end_index)){
url <- paste0(pre_url,i,html_url)
links_data_title <- read_html(url) %>% html_nodes(".title") %>% html_text()
links_data_url <- read_html(url) %>% html_nodes(".title a") %>% html_attr('href')
#post_count = post_count + length(links_data_title)
links_data_title <- str_replace_all(links_data_title,"\t","") %>% str_replace_all("\n","")
#custom setting========
flag = xor(grepl("管",links_data_title)|grepl("校長",links_data_title),      #包含的關鍵字
grepl("管理",links_data_title)|grepl("管學院",links_data_title))  #除去的關鍵字
#======================程式會根據標題裡面的關鍵字做抓取
#guan_count = guan_count + length(links_data_title[flag])
for(j in 1:length(links_data_title[flag])){
links_url = paste0('https://www.ptt.cc',links_data_url[j])
content_css = read_html(links_url) %>% html_nodes("#main-content") %>% html_text()
ptt_text = c(ptt_text, iconv(content_css,'utf8'))
}
}
#custom option===
end_index = 2360        #終止頁數#2360為2018年的開始頁數
latest_index = 2460     #最新頁數
#================程式將會從最新頁數一直抓到終止頁數
#關鍵字設定請看第31行
pre_url = "https://www.ptt.cc/bbs/NTU/index"
html_url = ".html"
#post_count = 0
guan_count = 0
ptt_text = c()
#custom option===
end_index = 2360        #終止頁數#2360為2018年的開始頁數
latest_index = 2415     #最新頁數
#================程式將會從最新頁數一直抓到終止頁數
#關鍵字設定請看第31行
pre_url = "https://www.ptt.cc/bbs/NTU/index"
html_url = ".html"
#post_count = 0
guan_count = 0
ptt_text = c()
for(i in c(latest_index:end_index)){
url <- paste0(pre_url,i,html_url)
links_data_title <- read_html(url) %>% html_nodes(".title") %>% html_text()
links_data_url <- read_html(url) %>% html_nodes(".title a") %>% html_attr('href')
#post_count = post_count + length(links_data_title)
links_data_title <- str_replace_all(links_data_title,"\t","") %>% str_replace_all("\n","")
#custom setting========
flag = xor(grepl("管",links_data_title)|grepl("校長",links_data_title),      #包含的關鍵字
grepl("管理",links_data_title)|grepl("管學院",links_data_title))  #除去的關鍵字
#======================程式會根據標題裡面的關鍵字做抓取
#guan_count = guan_count + length(links_data_title[flag])
for(j in 1:length(links_data_title[flag])){
links_url = paste0('https://www.ptt.cc',links_data_url[j])
content_css = read_html(links_url) %>% html_nodes("#main-content") %>% html_text()
ptt_text = c(ptt_text, iconv(content_css,'utf8'))
}
}
write.table(ptt_text, file = "text_content.txt", sep = " ")
xor(F,F)
grepl("管學院","校園徵才")
i in c(1,3,5,7)
for(i in c(1,3,5,7)){}
print(i)
links_data_title[flag]
links_data_title
flag
for(i in (1:5)[c(T,F,F,T,T)])
print(i)
library(xml2)
library(rvest)
library(stringr)
#custom option===
end_index = 2360        #終止頁數#2360為2018年的開始頁數
latest_index = 2415     #最新頁數
#================程式將會從最新頁數一直抓到終止頁數
#關鍵字設定請看第31行
pre_url = "https://www.ptt.cc/bbs/NTU/index"
html_url = ".html"
#post_count = 0
guan_count = 0
ptt_text = c()
for(i in c(latest_index:end_index)){
url <- paste0(pre_url,i,html_url)
links_data_title <- read_html(url) %>% html_nodes(".title") %>% html_text()
links_data_url <- read_html(url) %>% html_nodes(".title a") %>% html_attr('href')
#post_count = post_count + length(links_data_title)
links_data_title <- str_replace_all(links_data_title,"\t","") %>% str_replace_all("\n","")
#custom setting========
flag = xor(grepl("管",links_data_title)|grepl("校長",links_data_title),      #包含的關鍵字
grepl("管理",links_data_title)|grepl("管學院",links_data_title))  #除去的關鍵字
#======================程式會根據標題裡面的關鍵字做抓取
#guan_count = guan_count + length(links_data_title[flag])
for(j in (1:length(links_data_title))[flag]){
links_url = paste0('https://www.ptt.cc',links_data_url[j])
content_css = read_html(links_url) %>% html_nodes("#main-content") %>% html_text()
ptt_text = c(ptt_text, iconv(content_css,'utf8'))
}
}
write.table(ptt_text, file = "text_content.txt", sep = " ")
library(NLP)
library(readtext)
library(tm)
library(jiebaR)
library(jiebaRD)
content = readtext("text_content.txt")
content
content$doc_id
content$text
str(content$text)
docs <- Corpus(VectorSource(content$text))
docs
View(docs)
data = readtext("text_content.txt")
docs = Corpus(VectorSource(data$text))
help(gsub)
docs = Corpus(VectorSource(data$text))
toSpace <- content_transformer(function(x, pattern) {return (gsub(pattern, " ", x))})
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, toSpace, "[a-zA-Z]")
docs
docs$`1`
content(docs)
data = readtext("text_content.txt")
docs = Corpus(VectorSource(data$text))
toSpace <- content_transformer(function(x, pattern) {return (gsub(pattern, "", x))})
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, toSpace, "[a-zA-Z]")
content(docs)
docs <- tm_map(docs, toSpace, "[推噓a-zA-Z]")
content(docs)
docs <- tm_map(docs, "", "[ →推噓a-zA-Z]")
docs <- tm_map(docs, toSpace, "[ →推噓a-zA-Z]")
content(docs)
help(worker)
library(NLP)
library(tm)
library(stats)
library(proxy)
library(dplyr)
library(readtext)
library(jiebaRD)
library(jiebaR)
library(slam)
library(Matrix)
library(tidytext)
setwd("~/Desktop/CSX_4001_106-2/week_5/task_5")
rawData = readtext("*.txt")
docs = Corpus(VectorSource(rawData$text))
# data clean
toSpace <- content_transformer(function(x, pattern) {
return (gsub(pattern, " ", x))
})
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, toSpace, "[a-zA-Z]")
# words cut
keywords = read.csv("keywords.csv")
# words cut
keywords = read.csv("keywords.csv",fileEncoding = 'big5')
mixseg = worker()
keys = as.matrix(keywords)
new_user_word(mixseg, keys)
keys
mixseg
jieba_tokenizer = function(d){
unlist(segment(d[[1]], mixseg))
}
seg = lapply(docs, jieba_tokenizer)
seg
View(seg)
library(NLP)
library(tm)
library(stats)
library(proxy)
library(dplyr)
library(readtext)
library(jiebaRD)
library(jiebaR)
library(slam)
library(Matrix)
library(tidytext)
rawData = readtext("*.txt")
docs = Corpus(VectorSource(rawData$text))
# data clean
toSpace <- content_transformer(function(x, pattern) {
return (gsub(pattern, " ", x))
})
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, toSpace, "[a-zA-Z]")
docs
docs
keywords = read.csv("keywords.csv",fileEncoding = 'big5')
mixseg = worker()
keys = as.matrix(keywords)
new_user_word(mixseg, keys)
mixseg
keys
jieba_tokenizer = function(d){
unlist(segment(d[[1]], mixseg))
}
keywords
str(keywords)
keyword$兆利
keywords$兆利
keys
help("new_user_word")
new_user_word(mixseg, keys)
new_user_word()
mixseg
docs
docs[1]
docs[[1]
]
docs[[1]]
docs$`1`
docs$`2`
docs$`3`
content(docs)
# words cut
keywords = read.csv("keywords.csv",fileEncoding = 'big5')
rawData = readtext("*.txt")
docs = Corpus(VectorSource(rawData$text))
# data clean
toSpace <- content_transformer(function(x, pattern) {
return (gsub(pattern, " ", x))
})
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, toSpace, "[a-zA-Z]")
docs
content(docs)
rawData = readtext("*.txt",fileEncoding = 'big5')
rawData = readtext("*.txt", fileEncoding = 'utf8')
rawData = readtext("*.txt", Encoding = 'big5')
setwd("~/Desktop/CSX_4001_106-2/week_5/task_5")
rawData = readtext("*.txt", fileEncoding = 'big5')
rawData = readtext("*.txt")
docs = Corpus(VectorSource(rawData$text))
# data clean
toSpace <- content_transformer(function(x, pattern) {
return (gsub(pattern, " ", x))
})
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
library(NLP)
library(tm)
library(stats)
library(proxy)
library(dplyr)
library(readtext)
library(jiebaRD)
library(jiebaR)
library(slam)
library(Matrix)
library(tidytext)
rawData = readtext("*.txt")
docs = Corpus(VectorSource(rawData$text))
# data clean
toSpace <- content_transformer(function(x, pattern) {
return (gsub(pattern, " ", x))
})
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, toSpace, "[a-zA-Z]")
content(docs)
